Paso 1. 


Paso 2. Carga bucket s3
cree archivo .csv con los datos requeridos
configuré aws y demas credenciales en mi consola VSc
configuré servicio de CloudWhatch para advertencias de gastos
aseguré mi usuario root con MFA
creé un usuario IAM solo con permisos sobre s3
creé claves de accesos para este usuario IAM
creé un bucket desde AWS y cargué el archivo csv a travez de VS mediante las claves de accesos del usuario IAM
creé un bucket de prueba a travez de la consola VSc
DATOS:
    - example_data.csv (archivo.csv)
    - > aws configure
        - clave de acceso
        - clave secreta
        - region
        - formato
    - us_s3 (usuario IAM)
    - telecom-datalake-s3 (bucket)
    - aws s3 cp example_data.csv s3://telecom-datalake/data/ (carga en bucket)
    - aws s3 ls s3://telecom-datalake/data/  (verificar carga)
    - aws s3api create-bucket --bucket telecom-datalake-prueba2 --region us-east-1  (crear por consola)

Paso 3.
a) AWS Glue, Crear Crawler
Usé el servicio de AWS Glue
Creé un Crawler que apunte al archivo de mi bucket s3
Configuré un rol de IAM con permisos sobre s3 y Glue
Creé la base de datos de salida
Revisé y Ejecuté el Crawler
b) Creé una carpeta en mi bucket donde guardar un script de glue_etl_script
creé el script .py en VSc y lo guardé en el destino creado.

DATOS: 
    - crawler-clientes-s3  (crawler)
    - s3://telecom-datalake-s3/data/  (ruta de archivo .csv)
    - telecom_db  (base de datos)
    - scripts/ (carpeta nueva)
    - glue_etl_scripts.py (script en VSc)
    - aws s3 cp glue_etl_script.py s3://telecom-datalake-s3/scripts/ (Subir desde VSc)
    - 

Paso 4. Amazon Redshift
