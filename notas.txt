Paso 1. Configurar Entorno Local
    Istalar:Python 3.x,
            AWS CLI,
            pip install boto3  -> import boto3
            Git (configurar)
    En VSc: extensiones: Python (verificar interprete: ultima Version)
                        AWS toolkit (configurar credeciales)
                        git (GitLens)      
Paso 2. Carga bucket s3
    cree archivo 'example_data.csv' con los datos requeridos
    configuré servicio de CloudWhatch en AWS para advertencias de gastos
    aseguré mi usuario root con MFA
    creé un usuario IAM (user_s3-carga-data) solo con permisos personalizado para cargas en s3 (permisos_de_carga_en_s3.json)
        creé claves de accesos para este usuario IAM, y lo configuré para VSc
    creé un bucket desde VS code y subí el archivo 'example_data.csv' a travez de la consola de VS 
        'aws s3 cp example_data.csv s3://telecom-datalake-first/data/'
    creé un bucket de prueba a travez de AWS (bucket-prueba-mario)

Paso 3.Configurar AWS Glue
a) AWS Glue, Crear Crawler
Configuré un rol de IAM con permisos sobre s3, Glue y IAM (para poder crear rol de glue)
Usé el servicio de AWS Glue
Creé un Crawler que apunte al archivo de mi bucket s3
Creé la base de datos de salida
Revisé y Ejecuté el Crawler
b) Creé una carpeta en mi bucket donde guardar un script (glue_etl_script.py)
creé el script .py en VSc y lo subí en el destino creado atravez de AWS CLI.
    > aws s3 cp glue_etl_script.py s3://telecom-datalake-first/scripts/

creé carpeta 'transformed_data' como destino para el proximo paso.



DATOS: 
    - crawler-clientes-s3  (crawler)
    - s3://telecom-datalake-s3/data/  (ruta de archivo .csv)
    - telecom_db  (base de datos)
    - scripts/ (carpeta nueva)
    - glue_etl_scripts.py (script en VSc)
    - aws s3 cp glue_etl_script.py s3://telecom-datalake-s3/scripts/ (Subir desde VSc)
    - 

Paso 4. Amazon Redshift
